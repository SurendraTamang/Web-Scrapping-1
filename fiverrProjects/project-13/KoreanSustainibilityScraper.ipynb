{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkAGxj2MpLzg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#--Importing Selenium packages--#\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from scrapy import Selector\n",
    "from selenium_stealth import stealth\n",
    "\n",
    "#--Importing other required packages--#\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# import concurrent.futures\n",
    "\n",
    "\n",
    "# Function to download the pdf file\n",
    "def downlaodPdfs(data):\n",
    "  fileUrl = data.split(\" | \")[0]\n",
    "  fileName = data.split(\" | \")[1]\n",
    "  msg = True\n",
    "\n",
    "  #--Fetching data as byte streams to handle downloading of large size files--#\n",
    "  r = requests.get(fileUrl, stream = True)\n",
    "  #--Creating a pdf file--#\n",
    "  with open(f\"downloads/{fileName}\",\"wb\") as pdf:\n",
    "    #--Divide the received data into splits of 1MB & loop over it--#\n",
    "    for indx,chunk in enumerate(r.iter_content(chunk_size=1024)):\n",
    "      #--If the 1st chunk of the stream bytes doesn't have the PDF extension, then skip it & set the msg flag as false--#\n",
    "      if indx == 0 and b'%PDF' not in chunk:               \n",
    "        msg = False\n",
    "        break\n",
    "      #--If the 1st chunk of the stream bytes have the PDF extension, then write the data to the pdf file--#\n",
    "      else: \n",
    "        pdf.write(chunk)\n",
    "  #--If msg flag is false, which means the pdf doesn't have any data. So delete the pdf file that is being created--#\n",
    "  if not msg:\n",
    "    print(\"Skipping the file since it is empty!\\n\") \n",
    "    os.remove(f\"downloads/{fileName}\")\n",
    "  else:\n",
    "    print(f\"{fileName} downloaded successfully!\\n\")\n",
    "  return msg\n",
    "\n",
    "#--Function to write each row data(which is in the form of a dictionary) to a csv file--#\n",
    "def writeCSV(data, fieldName, FILE_NAME):\n",
    "    fileExists = os.path.isfile(FILE_NAME)\n",
    "    with open(FILE_NAME, 'a', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldName, lineterminator='\\n')\n",
    "        #--Funvtion to check if file exists, then skip the column names and if not then write the column names to the csv file--#\n",
    "        #--By This we are making sure, the column names are only written once to the csv file--# \n",
    "        if not fileExists:\n",
    "            writer.writeheader()\n",
    "        #--Insert the dictionary data to a new line in the csv--#\n",
    "        writer.writerow(data)\n",
    "\n",
    "# Function to fetch the Image code for column 7 - Standard\n",
    "def getImageCode(imgUrlLi):\n",
    "  li = []\n",
    "  for imgUrl in imgUrlLi:\n",
    "    if \"ico_use2.gif\" in imgUrl:\n",
    "      li.append('1')\n",
    "    elif \"ico_use7.gif\" in imgUrl:\n",
    "      li.append('2')\n",
    "    elif \"ico_use3.gif\" in imgUrl:\n",
    "      li.append('3')\n",
    "    elif \"ico_use8.gif\" in imgUrl:\n",
    "      li.append('4')\n",
    "  return li\n",
    "\n",
    "\n",
    "# Make sure you are using Python 3.5 and above\n",
    "# Run the requirements.txt first file to install the packages. Open the terminal & run the below command\n",
    "# (On Windows): pip install requirements.txt\n",
    "# (On Mac/Linux): pip3 install requirements.txt\n",
    "# Check your google chrome version & download the respective chromedriver from here: https://chromedriver.chromium.org/downloads\n",
    "# If your chromedriver is in the same folder as that of this .ipynb file, then use the path => \"./chromedriver\"\n",
    "\n",
    "CHROMEDRIVER_PATH = \"./chromedriver\"\n",
    "# CHROMEDRIVER_PATH = \"chromedriver\"\n",
    "\n",
    "#--Column Names--#\n",
    "FIELD_NAMES = ['Sl No', 'Company Name', 'Start Time', 'End Time', 'Published Date', 'Industry', 'Standard', 'fileNameWeb', 'fileNamePdf', 'fileNameText']\n",
    "\n",
    "FILE_NAME_WITH_BLANKS = \"data1.csv\"    #--File name for data containg Blank PDFs\n",
    "FILE_NAME_WITHOUT_BLANKS = \"data2.csv\" #--File name for data not containg Blank PDFs\n",
    "page_num = 1\n",
    "slNoWB = 0                             #--Serial number for data containg Blank PDFs\n",
    "slNoWOB = 1                            #--Serial number for data not containg Blank PDFs\n",
    "# pdfDwnldUrlLi = []\n",
    "\n",
    "#--Create downloads folder if not exists to save the PDF files--#\n",
    "if not (os.path.isdir(\"./downloads\")):\n",
    "  os.mkdir(\"./downloads\")\n",
    "\n",
    "#--Initializing the chrome driver--#\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument(\"start-maximized\")\n",
    "\n",
    "#  --Uncomment the below two lines if running on linux / Mac--  #\n",
    "# chrome_options.add_argument('--no-sandbox')\n",
    "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "driver = webdriver.Chrome(CHROMEDRIVER_PATH, chrome_options=chrome_options)\n",
    "\n",
    "stealth(driver,\n",
    "        languages=[\"en-US\", \"en\"],\n",
    "        vendor=\"Google Inc.\",\n",
    "        platform=\"Win32\",\n",
    "        webgl_vendor=\"Intel Inc.\",\n",
    "        renderer=\"Intel Iris OpenGL Engine\",\n",
    "        fix_hairline=True,\n",
    "        )\n",
    "\n",
    "# Open chrome browser in the background and visit the url\n",
    "driver.get(\"https://www.ksa.or.kr/ksi/4982/subview.do?enc=Zm5jdDF8QEB8JTJGa3JjYSUyRmtzaSUyRjElMkZ2aWV3LmRvJTNGcGFnZSUzRDElMjZzcmNoQ29sdW1uJTNEJTI2c3JjaFdyZCUzRCUyNnNvcnRDb2x1bW4lM0QlMjY%3D\")\n",
    "# Wait till the page is fully loaded\n",
    "WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.XPATH, \"//tbody[last()]/tr\")))\n",
    "\n",
    "# Looping over the 156 pages\n",
    "while True:\n",
    "  page_num += 1\n",
    "  # Extracting the HTML DOM\n",
    "  html = driver.page_source\n",
    "  respObj = Selector(text=html)\n",
    "\n",
    "  # Looping over the data in each page\n",
    "  dwnldFiles = respObj.xpath(\"//tbody[last()]/tr\")\n",
    "  for file in dwnldFiles:\n",
    "    slNoWB += 1\n",
    "\n",
    "    #--Xpath expressions to extract data from the HTML DOM--#\n",
    "    companyName = file.xpath(\"normalize-space(.//td[1]/text())\").get()\n",
    "    startEndDate = file.xpath(\"normalize-space(.//td[2]/text())\").get()    \n",
    "    startDate = startEndDate.split(\"~\")[0]\n",
    "    endDate = startEndDate.split(\"~\")[1]\n",
    "    fileNamePdf_WB = f'''{slNoWB}_{companyName}_{startDate.split(\".\")[0]}_{endDate.split(\".\")[0]}'''\n",
    "    fileNamePdf_WOB = f'''{slNoWOB}_{companyName}_{startDate.split(\".\")[0]}_{endDate.split(\".\")[0]}'''\n",
    "    pdfUrl = f'''https://www.ksa.or.kr{file.xpath(\".//td/a/@href\").get()}'''\n",
    "    # pdfDwnldUrlLi.append(f'''{pdfUrl} | {fileNamePdf}.pdf''')\n",
    "\n",
    "    # Saving each row data as a dictionary\n",
    "    dataDict = {\n",
    "        FIELD_NAMES[0]: slNoWB,\n",
    "        FIELD_NAMES[1]: companyName,\n",
    "        FIELD_NAMES[2]: startDate,\n",
    "        FIELD_NAMES[3]: endDate,\n",
    "        FIELD_NAMES[4]: file.xpath(\"normalize-space(.//td[3]/text())\").get(),\n",
    "        FIELD_NAMES[5]: file.xpath(\"normalize-space(.//td[4]/text())\").get(),\n",
    "        FIELD_NAMES[6]: \",\".join(getImageCode(file.xpath(\".//td[5]/img/@src\").getall())),\n",
    "        FIELD_NAMES[7]: file.xpath(\"normalize-space(.//td/a/text())\").get(),\n",
    "        FIELD_NAMES[8]: fileNamePdf_WB,\n",
    "        FIELD_NAMES[9]: fileNamePdf_WB\n",
    "    }\n",
    "    print(dataDict)\n",
    "\n",
    "    #--Write the dictionary to the csv file--#\n",
    "    writeCSV(dataDict, FIELD_NAMES, FILE_NAME_WITH_BLANKS)\n",
    "\n",
    "    #--Logic to skip the rows / pdf file that don't contain nay data\n",
    "    if downlaodPdfs(f'''{pdfUrl} | {fileNamePdf_WOB}.pdf'''):\n",
    "      dataDict['Sl No'] = slNoWOB\n",
    "      dataDict['fileNamePdf'] = fileNamePdf_WOB\n",
    "      dataDict['fileNameText'] = fileNamePdf_WOB\n",
    "      writeCSV(dataDict, FIELD_NAMES, FILE_NAME_WITHOUT_BLANKS)\n",
    "      slNoWOB += 1\n",
    "\n",
    "  #--Handling the pagination--#\n",
    "  nextPage1 = respObj.xpath(f\"//div[contains(@class, 'paging')]//ul/li/a[text()='{page_num}']\")\n",
    "  nextPage2 = respObj.xpath(\"//a[@class='_next']\")\n",
    "  if nextPage1:\n",
    "    nextPageElem = driver.find_element_by_xpath(f\"//div[contains(@class, 'paging')]//ul/li/a[text()='{page_num}']\")\n",
    "    driver.execute_script(\"arguments[0].click()\", nextPageElem)\n",
    "    time.sleep(2)\n",
    "    WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.XPATH, \"//tbody[last()]/tr\")))\n",
    "  elif nextPage2:\n",
    "    nextPageElem = driver.find_element_by_xpath(\"//a[@class='_next']\")\n",
    "    driver.execute_script(\"arguments[0].click()\", nextPageElem)\n",
    "    time.sleep(2)\n",
    "    WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.XPATH, \"//tbody[last()]/tr\")))\n",
    "  else:\n",
    "    break\n",
    "\n",
    "#--Shutting down chrome once all the data are being scrapped--#\n",
    "driver.quit()\n",
    "  \n",
    "# print(\"\\n\\nDownloading PDFs...\\n\\n\")\n",
    "\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#   executor.map(downlaodPdfs, pdfDwnldUrlLi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MN1I6BRSg-AV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "KoreanSustainibilityScraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}